{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Knowledge Tracing.ipynb\n",
    "\n",
    "This file contains the code for the inference using the GPT-3.5 API in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and helper codes\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import pandas\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "import tiktoken\n",
    "\n",
    "from gpt_helpers import *\n",
    "\n",
    "should_include_KCs = True\n",
    "should_include_KC_texts = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data and topics (= KC) for our inference\n",
    "\n",
    "generate_new_file('dataverse_files/2_DBE_KT22_datafiles_100102_csv/KCs.csv',\n",
    "                  'dataverse_files/2_DBE_KT22_datafiles_100102_csv/Question_KC_Relationships.csv',\n",
    "                  'dataverse_files/2_DBE_KT22_datafiles_100102_csv/Generated_KC_Questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue preparing the data\n",
    "\n",
    "data = read_data('', # change with the test data file you want to use for inference (e.g. the downsampled data)\n",
    "                    'dataverse_files/2_DBE_KT22_datafiles_100102_csv/Questions.csv',\n",
    "                    'dataverse_files/2_DBE_KT22_datafiles_100102_csv/Generated_KC_Questions.csv',\n",
    "                    N = -1)\n",
    "data = remove_padding(data) # kept for compatibility with the original code, which added padding in case of unequal-length subsequences. (The current code keeps all subsequences of the same length.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API information for the OpenAI API. Replace with your own information (kept empty for security reasons)\n",
    "\n",
    "api_info = {\n",
    "    'api_key': \"\",\n",
    "    'api_version': \"\",\n",
    "    'azure_endpoint': \"\",\n",
    "    'model': \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prompts to be used for GPT-3.5 inference\n",
    "prompts, gts = generate_prompts(data, incl_id = False, incl_q = False, incl_kc = should_include_KCs, incl_diff = True)\n",
    "prompts_sample, gts_sample = randomly_sample_prompts(prompts, gts, N = len(prompts), seed = 0, max_token_len = 4096) # in the original code, we randomly \"sampled\" prompts, but for final run, this function effectively only shuffles the prompts and keeps those that fit within the maximum context length of our model\n",
    "\n",
    "# Generate predictions from the GPT-3.5 model\n",
    "preds = predict(prompts_sample, gts_sample, api_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions of the GPT-3.5 model\n",
    "metrics = evaluate(preds, gts_sample)\n",
    "\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
