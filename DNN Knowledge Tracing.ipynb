{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Knowledge Tracing.ipynb\n",
    "\n",
    "This file contains the code for the training the DNNs in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and helper codes\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from seq_helpers import *\n",
    "\n",
    "# Config variables to downsample and/or include KCs\n",
    "should_downsample = False\n",
    "should_include_kcs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") # change to cuda or cpu if necessary\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "We perform a grid search over N (the sequence length in the input data) and n_layers (the number of layers in each model). We select only these two changes, because we want to approximate the effect of varying the input sequence length on model performance (data source variation) AND the effect of increasing model complexity via n_layers. We could have chosen a different complexity hyperparameter (like hidden_size), but opt to choose only one due to computational constraints (adding one extra hyperparameter to the search increased our costs significantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the parameters if necessary\n",
    "hparams = {\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 100,\n",
    "    'input_size': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Dictionary to store the best models. acc = normal accuracy, bal_acc = balanced accuracy, f1_0 = f1 score for label 0 (wrong), f1_1 = f1 score for label 1 (correct), sup_0 = support for label 0 (wrong), sup_1 = support for label 1 (correct), auc = AUC score, epoch = epoch number in which the best model was found\n",
    "best_metrics = {\n",
    "    'RNN': {\n",
    "        'val': {'acc': 0, 'bal_acc': 0, 'f1_0': 0, 'f1_1': 0, 'sup_0': 0, 'sup_1': 0, 'auc': 0, 'epoch': 0},\n",
    "        'test': {'acc': 0, 'bal_acc': 0, 'f1_0': 0, 'f1_1': 0, 'sup_0': 0, 'sup_1': 0, 'auc': 0, 'epoch': 0}\n",
    "        },\n",
    "    'LSTM': {\n",
    "        'val': {'acc': 0, 'bal_acc': 0, 'f1_0': 0, 'f1_1': 0, 'sup_0': 0, 'sup_1': 0, 'auc': 0, 'epoch': 0},\n",
    "        'test': {'acc': 0, 'bal_acc': 0, 'f1_0': 0, 'f1_1': 0, 'sup_0': 0, 'sup_1': 0, 'auc': 0, 'epoch': 0}\n",
    "        },\n",
    "    'Transformer': {\n",
    "        'val': {'acc': 0, 'bal_acc': 0, 'f1_0': 0, 'f1_1': 0, 'sup_0': 0, 'sup_1': 0, 'auc': 0, 'epoch': 0},\n",
    "        'test': {'acc': 0, 'bal_acc': 0, 'f1_0': 0, 'f1_1': 0, 'sup_0': 0, 'sup_1': 0, 'auc': 0, 'epoch': 0}\n",
    "        },\n",
    "}\n",
    "\n",
    "for N in [5, 10, 20]:\n",
    "    for n_layers in [1, 2, 5]:\n",
    "        for model_name in ['RNN', 'LSTM', 'Transformer']:\n",
    "            print(f\"Training {model_name} with N = {N} and n_layers = {n_layers}\")\n",
    "\n",
    "            # Load data\n",
    "            train_data = load_and_process_data(f'data_outputs/train_fold_1_n_{N}.json', seq_len = N)\n",
    "            val_data = load_and_process_data(f'data_outputs/val_fold_1_n_{N}.json', seq_len = N)\n",
    "            test_data = load_and_process_data(f'data_outputs/test_n_{N}.json', seq_len = N)\n",
    "\n",
    "            train_dataset = StudentSequenceDataset(train_data, embedding_dim=0)\n",
    "            val_dataset = StudentSequenceDataset(val_data, embedding_dim=0)\n",
    "            test_dataset = StudentSequenceDataset(test_data, embedding_dim=0)\n",
    "\n",
    "            train_data_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "            val_data_loader = DataLoader(val_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "            test_data_loader = DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "            # Create the model and evaluator\n",
    "            if model_name == 'RNN':\n",
    "                model = SimpleRNN(input_size=hparams['input_size'], num_layers=n_layers).to(device)\n",
    "            elif model_name == 'LSTM':\n",
    "                model = SimpleLSTM(input_size=hparams['input_size'], num_layers=n_layers).to(device)\n",
    "            elif model_name == 'Transformer':\n",
    "                model = SimpleTransformer(input_size=hparams['input_size'], num_layers=n_layers).to(device)\n",
    "            evaluator = ModelEvaluator({f'{model_name}_{N}_{n_layers}_epoch': model}, train_data_loader, val_data_loader, test_data_loader, device, epochs=hparams['epochs'], lr=hparams['lr'])\n",
    "            \n",
    "            # Train and evaluate the model\n",
    "            evaluator.train_and_evaluate()\n",
    "            val_metrics, test_metrics = evaluator.compute_best_metrics()\n",
    "\n",
    "            # Check if the model is the best one\n",
    "            if val_metrics['bal_acc'] > best_metrics[model_name]['val']['bal_acc']:\n",
    "                best_metrics[model_name]['val'] = val_metrics\n",
    "                best_metrics[model_name]['test'] = test_metrics\n",
    "                print(f\"New best {model_name} model with N = {N} and n_layers = {n_layers}!\")\n",
    "                print(f\"    Val: {val_metrics}\")\n",
    "                print(f\"    Test: {test_metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Score Demo\n",
    "We select one of our best models (RNN with N=5 and n_layers=5), and use it to both predict the correctness of a random student's answer as well as a confidence in this prediction (approximated by the raw sigmoid output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 100,\n",
    "    'input_size': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RNN'\n",
    "N = 5\n",
    "n_layers = 5\n",
    "\n",
    "print(f\"Training {model_name} with N = {N} and n_layers = {n_layers}\")\n",
    "\n",
    "# Load data\n",
    "\n",
    "train_data = load_and_process_data(f'data_outputs/train_fold_1_n_{N}.json', seq_len = N)\n",
    "val_data = load_and_process_data(f'data_outputs/val_fold_1_n_{N}.json', seq_len = N)\n",
    "test_data = load_and_process_data(f'data_outputs/test_n_{N}.json', seq_len = N)\n",
    "\n",
    "train_dataset = StudentSequenceDataset(train_data, embedding_dim=0)\n",
    "val_dataset = StudentSequenceDataset(val_data, embedding_dim=0)\n",
    "test_dataset = StudentSequenceDataset(test_data, embedding_dim=0)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "# Create the model and evaluator\n",
    "\n",
    "model = SimpleRNN(input_size=hparams['input_size'], num_layers=n_layers).to(device)\n",
    "\n",
    "evaluator = ModelEvaluator({f'{model_name}_{N}_{n_layers}_epoch': model}, train_data_loader, val_data_loader, test_data_loader, device, epochs=hparams['epochs'], lr=hparams['lr'])\n",
    "\n",
    "# Train and evaluate the model\n",
    "\n",
    "evaluator.train_and_evaluate()\n",
    "val_metrics, test_metrics = evaluator.compute_best_metrics(save_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 1.0\n",
      "Prediction: 1.0\n",
      "Confidence: 99.99145269393921%\n",
      "Verdict: Balanced\n",
      "\n",
      "True Label: 0.0\n",
      "Prediction: 0.0\n",
      "Confidence: 0.0017738397218636237%\n",
      "Verdict: Underconfident\n",
      "\n",
      "True Label: 1.0\n",
      "Prediction: 1.0\n",
      "Confidence: 99.18718338012695%\n",
      "Verdict: Balanced\n",
      "\n",
      "True Label: 1.0\n",
      "Prediction: 1.0\n",
      "Confidence: 99.9940276145935%\n",
      "Verdict: Balanced\n",
      "\n",
      "True Label: 1.0\n",
      "Prediction: 1.0\n",
      "Confidence: 99.95296001434326%\n",
      "Verdict: Balanced\n",
      "\n",
      "True Label: 1.0\n",
      "Prediction: 1.0\n",
      "Confidence: 99.92734789848328%\n",
      "Verdict: Balanced\n",
      "\n",
      "True Label: 0.0\n",
      "Prediction: 0.0\n",
      "Confidence: 3.0384546789719025e-05%\n",
      "Verdict: Underconfident\n",
      "\n",
      "True Label: 1.0\n",
      "Prediction: 1.0\n",
      "Confidence: 99.97549653053284%\n",
      "Verdict: Balanced\n",
      "\n",
      "True Label: 0.0\n",
      "Prediction: 1.0\n",
      "Confidence: 99.97177720069885%\n",
      "Verdict: Overconfident\n",
      "\n",
      "True Label: 1.0\n",
      "Prediction: 1.0\n",
      "Confidence: 99.90384578704834%\n",
      "Verdict: Balanced\n",
      "\n",
      "True Label: 1.0\n",
      "Prediction: 0.0\n",
      "Confidence: 0.8072282187640667%\n",
      "Verdict: Balanced\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with confidence score\n",
    "\n",
    "labels, preds, confs = evaluator.predict(f'{model_name}_{N}_{n_layers}_epoch', test_data_loader, device, with_confidence=True)\n",
    "\n",
    "# Print 10 predictions and their confidences (see the Ethical Risk Assessment section in the report)\n",
    "for i in range(55, 66):\n",
    "    print(f\"True Label: {labels[i]}\")\n",
    "    print(f\"Prediction: {preds[i]}\")\n",
    "    print(f\"Confidence: {confs[i] * 100}%\")\n",
    "    if labels[i] != preds[i]:\n",
    "        if confs[i] > 0.9:\n",
    "            print(\"Verdict: Overconfident\\n\")\n",
    "        else:\n",
    "            print(\"Verdict: Balanced\\n\")\n",
    "    \n",
    "    if labels[i] == preds[i]:\n",
    "        if confs[i] < 0.1:\n",
    "            print(\"Verdict: Underconfident\\n\")\n",
    "        else:\n",
    "            print(\"Verdict: Balanced\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
